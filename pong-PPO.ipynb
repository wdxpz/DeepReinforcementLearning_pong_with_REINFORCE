{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome!\n",
    "Below, we will learn to implement and train a policy to play atari-pong, using only the pixels as input. We will use convolutional neural nets, multiprocessing, and pytorch to implement and train our policy. Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: JSAnimation in /usr/local/lib/python3.7/site-packages (0.1)\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "using device:  cpu\n"
     ]
    }
   ],
   "source": [
    "# install package for displaying animation\n",
    "!pip install JSAnimation\n",
    "\n",
    "# custom utilies for displaying animation, collecting rollouts and more\n",
    "import pong_utils\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# check which device is being used. \n",
    "# I recommend disabling gpu until you've made sure that the code runs\n",
    "device = pong_utils.device\n",
    "print(\"using device: \",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of available actions:  ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "# render ai gym environment\n",
    "import gym\n",
    "import time\n",
    "\n",
    "# PongDeterministic does not contain random frameskip\n",
    "# so is faster to train than the vanilla Pong-v4 environment\n",
    "env = gym.make('PongDeterministic-v4')\n",
    "\n",
    "print(\"List of available actions: \", env.unwrapped.get_action_meanings())\n",
    "\n",
    "# we will only use the actions 'RIGHTFIRE' = 4 and 'LEFTFIRE\" = 5\n",
    "# the 'FIRE' part ensures that the game starts again after losing a life\n",
    "# the actions are hard-coded in pong_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "To speed up training, we can simplify the input by cropping the images and use every other pixel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# show what a preprocessed image looks like\n",
    "env.reset()\n",
    "_, _, _, _ = env.step(0)\n",
    "# get a frame after 20 steps\n",
    "for _ in range(20):\n",
    "    frame, _, _, _ = env.step(1)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(frame)\n",
    "plt.title('original image')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('preprocessed image')\n",
    "\n",
    "# 80 x 80 black and white image\n",
    "plt.imshow(pong_utils.preprocess_single(frame), cmap='Greys')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy\n",
    "\n",
    "## Exercise 1: Implement your policy\n",
    " \n",
    "Here, we define our policy. The input is the stack of two different frames (which captures the movement), and the output is a number $P_{\\rm right}$, the probability of moving left. Note that $P_{\\rm left}= 1-P_{\\rm right}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "# set up a convolutional neural net\n",
    "# the output is the probability of moving right\n",
    "# P(left) = 1-P(right)\n",
    "class Policy(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        \n",
    "        \n",
    "    ########\n",
    "    ## \n",
    "    ## Modify your neural network\n",
    "    ##\n",
    "    ########\n",
    "        \n",
    "        # 80x80 to outputsize x outputsize\n",
    "        # outputsize = (inputsize - kernel_size + stride)/stride \n",
    "        # (round up if not an integer)\n",
    "\n",
    "        # output = 20x20 here\n",
    "        self.con1 = nn.Conv2d(2, 8, kernel_size=4, stride=4)\n",
    "        #size becomes 20*20*8\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=4, stride=4)\n",
    "        self.size=16*5*5\n",
    "        \n",
    "        # 2 fully connected layer\n",
    "        self.fc1 = nn.Linear(self.size, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "    ########\n",
    "    ## \n",
    "    ## Modify your neural network\n",
    "    ##\n",
    "    ########\n",
    "    \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        # flatten the tensor\n",
    "        x = x.view(-1,self.size)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.sig(x)\n",
    "\n",
    "\n",
    "# run your own policy!\n",
    "# policy=Policy().to(device)\n",
    "policy=pong_utils.Policy().to(device)\n",
    "\n",
    "# we use the adam optimizer with learning rate 2e-4\n",
    "# optim.SGD is also possible\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game visualization\n",
    "pong_utils contain a play function given the environment and a policy. An optional preprocess function can be supplied. Here we define a function that plays a game and shows learning progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d3702a4fab9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpong_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# try to add the option \"preprocess=pong_utils.preprocess_single\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# to see what the agent sees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/project/deep-reinforcement-learning/pong-with-REINFORCE/pong_utils.py\u001b[0m in \u001b[0;36mplay\u001b[0;34m(env, policy, time, preprocess, nrand)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0manimate_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manim_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/project/deep-reinforcement-learning/pong-with-REINFORCE/pong_utils.py\u001b[0m in \u001b[0;36manimate_frames\u001b[0;34m(frames)\u001b[0m\n\u001b[1;32m     47\u001b[0m         lambda x: patch.set_data(frames[x]), frames = len(frames), interval=30)\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisplay_animation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfanim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'once'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# play a game and display the animation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/JSAnimation/IPython_display.py\u001b[0m in \u001b[0;36mdisplay_animation\u001b[0;34m(anim, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;34m\"\"\"Display the animation with an IPython HTML object\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manim_to_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/JSAnimation/IPython_display.py\u001b[0m in \u001b[0;36manim_to_html\u001b[0;34m(anim, fps, embed_frames, default_mode)\u001b[0m\n\u001b[1;32m     74\u001b[0m             anim.save(f.name,  writer=HTMLWriter(fps=fps,\n\u001b[1;32m     75\u001b[0m                                                  \u001b[0membed_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membed_frames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                                                  default_mode=default_mode))\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filename, writer, fps, dpi, codec, bitrate, extra_args, metadata, extra_anim, savefig_kwargs, progress_callback)\u001b[0m\n\u001b[1;32m   1154\u001b[0m                             \u001b[0mprogress_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m                             \u001b[0mframe_number\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m                     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrab_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0msavefig_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0;31m# Reconnect signal for first draw if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36msaving\u001b[0;34m(self, fig, outfile, dpi, *args, **kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36mfinish\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0;31m# are available to be assembled.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m         \u001b[0mMovieWriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Will call clean-up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36mfinish\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;34m'''Finish any processing for writing the movie.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgrab_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msavefig_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36mcleanup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m         \u001b[0mMovieWriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;31m# Delete temporary files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36mcleanup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_frame_sink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;31m# Use the encoding/errors that universal_newlines would use.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextIOWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m         \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextIOWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'"
     ]
    }
   ],
   "source": [
    "pong_utils.play(env, policy, time=200) \n",
    "# try to add the option \"preprocess=pong_utils.preprocess_single\"\n",
    "# to see what the agent sees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definitions\n",
    "Here you will define key functions for training. \n",
    "\n",
    "## Exercise 2: write your own function for training\n",
    "(what I call scalar function is the same as policy_loss up to a negative sign)\n",
    "\n",
    "### PPO\n",
    "Later on, you'll implement the PPO algorithm as well, and the scalar function is given by\n",
    "$\\frac{1}{T}\\sum^T_t \\min\\left\\{R_{t}^{\\rm future}\\frac{\\pi_{\\theta'}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)},R_{t}^{\\rm future}{\\rm clip}_{\\epsilon}\\!\\left(\\frac{\\pi_{\\theta'}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)}\\right)\\right\\}$\n",
    "\n",
    "the ${\\rm clip}_\\epsilon$ function is implemented in pytorch as ```torch.clamp(ratio, 1-epsilon, 1+epsilon)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipped_surrogate(policy, old_probs, states, actions, rewards,\n",
    "                      discount = 0.995, epsilon=0.1, beta=0.01):\n",
    "\n",
    "    ########\n",
    "    ## \n",
    "    ## WRITE YOUR OWN CODE HERE\n",
    "    ##\n",
    "    ########\n",
    "    trajectory_len = len(rewards)\n",
    "    discounts = discount**np.arange(trajectory_len)\n",
    "    discounted_rewards = np.asarray(rewards) * discounts[:, np.newaxis]\n",
    "    futher_discounted_rewards = discounted_rewards[::-1].cumsum(axis=0)[::-1]\n",
    "    #normalize the futher rewards\n",
    "    mean = np.mean(futher_discounted_rewards, axis=1)\n",
    "    std = np.std(futher_discounted_rewards, axis=1)+1.0e-10\n",
    "    normalized_rewards = (futher_discounted_rewards-mean)/std\n",
    "    \n",
    "    actions = torch.tensor(actions, dtype=torch.int8, device=device)\n",
    "    normalized_rewards = torch.tensor(normalized_rewards, dtype=torch.float, device=device)\n",
    "    states = torch.tensor(states, dtype=torch.int8, device=device)\n",
    "    old_probs = torch.tensor(old_probs, dtype=torch.float, device=device)\n",
    "\n",
    "    # convert states to policy (or probability)\n",
    "    new_probs = pong_utils.states_to_prob(policy, states)\n",
    "    new_probs = torch.where(actions == pong_utils.RIGHT, new_probs, 1.0-new_probs)\n",
    "    \n",
    "    ratio  = new_probs / (old_probs+1e-10)\n",
    "    clipped_ratio = torch.clamp(ratio, 1-epsilon, 1+epsilon)\n",
    "    \n",
    "\n",
    "    # include a regularization term\n",
    "    # this steers new_policy towards 0.5\n",
    "    # prevents policy to become exactly 0 or 1 helps exploration\n",
    "    # add in 1.e-10 to avoid log(0) which gives nan\n",
    "    entropy = -(new_probs*torch.log(old_probs+1.e-10)+ \\\n",
    "        (1.0-new_probs)*torch.log(1.0-old_probs+1.e-10))\n",
    "\n",
    "    return torch.mean(torch.min(ratio, clipped_ratio)*normalized_rewards + beta*entropy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "We are now ready to train our policy!\n",
    "WARNING: make sure to turn on GPU, which also enables multicore processing. It may take up to 45 minutes even with GPU enabled, otherwise it will take much longer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from parallelEnv import parallelEnv\n",
    "import numpy as np\n",
    "# keep track of how long training takes\n",
    "# WARNING: running through all 800 episodes will take 30-45 minutes\n",
    "\n",
    "# training loop max iterations\n",
    "episode = 500\n",
    "\n",
    "# widget bar to display progress\n",
    "!pip install progressbar\n",
    "import progressbar as pb\n",
    "widget = ['training loop: ', pb.Percentage(), ' ', \n",
    "          pb.Bar(), ' ', pb.ETA() ]\n",
    "timer = pb.ProgressBar(widgets=widget, maxval=episode).start()\n",
    "\n",
    "\n",
    "envs = parallelEnv('PongDeterministic-v4', n=8, seed=1234)\n",
    "\n",
    "discount_rate = .99\n",
    "epsilon = 0.1\n",
    "beta = .01\n",
    "tmax = 320\n",
    "SGD_epoch = 4\n",
    "\n",
    "# keep track of progress\n",
    "mean_rewards = []\n",
    "\n",
    "for e in range(episode):\n",
    "\n",
    "    # collect trajectories\n",
    "    old_probs, states, actions, rewards = \\\n",
    "        pong_utils.collect_trajectories(envs, policy, tmax=tmax)\n",
    "        \n",
    "    total_rewards = np.sum(rewards, axis=0)\n",
    "\n",
    "\n",
    "    # gradient ascent step\n",
    "    for _ in range(SGD_epoch):\n",
    "        \n",
    "        # uncomment to utilize your own clipped function!\n",
    "        # L = -clipped_surrogate(policy, old_probs, states, actions, rewards, epsilon=epsilon, beta=beta)\n",
    "\n",
    "        L = -pong_utils.clipped_surrogate(policy, old_probs, states, actions, rewards,\n",
    "                                          epsilon=epsilon, beta=beta)\n",
    "        optimizer.zero_grad()\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "        del L\n",
    "    \n",
    "    # the clipping parameter reduces as time goes on\n",
    "    epsilon*=.999\n",
    "    \n",
    "    # the regulation term also reduces\n",
    "    # this reduces exploration in later runs\n",
    "    beta*=.995\n",
    "    \n",
    "    # get the average reward of the parallel environments\n",
    "    mean_rewards.append(np.mean(total_rewards))\n",
    "    \n",
    "    # display some progress every 20 iterations\n",
    "    if (e+1)%20 ==0 :\n",
    "        print(\"Episode: {0:d}, score: {1:f}\".format(e+1,np.mean(total_rewards)))\n",
    "        print(total_rewards)\n",
    "        \n",
    "    # update progress widget bar\n",
    "    timer.update(e+1)\n",
    "    \n",
    "timer.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pong_utils.play(env, policy, time=200) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your policy!\n",
    "torch.save(policy, 'PPO.policy')\n",
    "\n",
    "# load policy if needed\n",
    "# policy = torch.load('PPO.policy')\n",
    "\n",
    "# try and test out the solution \n",
    "# make sure GPU is enabled, otherwise loading will fail\n",
    "# (the PPO verion can win more often than not)!\n",
    "#\n",
    "# policy_solution = torch.load('PPO_solution.policy')\n",
    "# pong_utils.play(env, policy_solution, time=2000) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
